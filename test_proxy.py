import akshare as ak
import requests
import random
import time
import os
from fake_useragent import UserAgent

class HybridProxyCrawler:
    def __init__(self, proxy_list=None):
        self.ua = UserAgent()
        self.proxy_list = proxy_list or [
            'http://127.0.0.1:7890'  # ä½ çš„ä»£ç†åœ°å€
        ]
        self.request_count = 0
        self.setup_session()
    
    def setup_session(self, use_proxy=None):
        """è®¾ç½®ä¼šè¯ï¼Œéšæœºå†³å®šæ˜¯å¦ä½¿ç”¨ä»£ç†"""
        self.request_count += 1
        
        # ğŸ¯ ç­–ç•¥1ï¼šéšæœºå†³å®šï¼ˆ70%ä¸ç”¨ä»£ç†ï¼Œ30%ç”¨ä»£ç†ï¼‰
        #if use_proxy is None:
        #    use_proxy = random.random() < 0.3
        
        # ğŸ¯ ç­–ç•¥2ï¼šæ¯Næ¬¡è¯·æ±‚åˆ‡æ¢ä¸€æ¬¡
        if self.request_count % 2 == 0:
            use_proxy = True
        else:
            use_proxy = False
        
        if use_proxy:
            # ä½¿ç”¨ä»£ç†
            proxy = random.choice(self.proxy_list)
            os.environ['HTTP_PROXY'] = proxy
            os.environ['HTTPS_PROXY'] = proxy
            print(f"ğŸ”€ ä½¿ç”¨ä»£ç†: {proxy}")
        else:
            # ä¸ç”¨ä»£ç†ï¼ˆæ¸…é™¤ä»£ç†è®¾ç½®ï¼‰
            os.environ.pop('HTTP_PROXY', None)
            os.environ.pop('HTTPS_PROXY', None)
            print("ğŸ  ä½¿ç”¨ç›´è¿")
        
        # è®¾ç½®çœŸå®çš„è¯·æ±‚å¤´
        self.setup_realistic_headers()
        
        return use_proxy
    
    def setup_realistic_headers(self):
        """è®¾ç½®çœŸå®çš„æµè§ˆå™¨è¯·æ±‚å¤´"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        ak.session = session
    
    def smart_delay(self):
        """æ™ºèƒ½å»¶è¿Ÿ"""
        # ç”¨ä»£ç†æ—¶å»¶è¿ŸçŸ­ä¸€äº›ï¼ˆä»£ç†æœåŠ¡å™¨é€šå¸¸è¾ƒæ…¢ï¼‰
        current_proxy = os.environ.get('HTTP_PROXY')
        if current_proxy:
            delay = random.uniform(20, 30)  # ä»£ç†æ¨¡å¼å»¶è¿ŸçŸ­
        else:
            delay = random.uniform(30, 40)  # ç›´è¿æ¨¡å¼å»¶è¿Ÿé•¿
        
        print(f"â³ ç­‰å¾… {delay:.1f} ç§’...")
        time.sleep(delay)
    
    def fetch_data(self, symbol, force_proxy=None):
        """è·å–æ•°æ®ï¼Œå¯å¼ºåˆ¶æŒ‡å®šæ˜¯å¦ç”¨ä»£ç†"""
        use_proxy = self.setup_session(use_proxy=force_proxy)
        
        try:
            df = ak.stock_zh_a_hist(symbol=symbol, period="daily")
            print(f"âœ… æˆåŠŸè·å– {symbol}")
            return df
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥: {e}")
            
            # å¤±è´¥æ—¶åˆ‡æ¢æ¨¡å¼é‡è¯•
            print("ğŸ”„ åˆ‡æ¢æ¨¡å¼é‡è¯•...")
            use_proxy = not use_proxy  # åˆ‡æ¢ä»£ç†/ç›´è¿æ¨¡å¼
            self.setup_session(use_proxy=use_proxy)
            
            try:
                df = ak.stock_zh_a_hist(symbol=symbol, period="daily")
                print(f"âœ… é‡è¯•æˆåŠŸè·å– {symbol}")
                return df
            except Exception as e2:
                print(f"âŒ é‡è¯•ä¹Ÿå¤±è´¥: {e2}")
                raise e2
        finally:
            self.smart_delay()

# ä½¿ç”¨ç¤ºä¾‹
#crawler = HybridProxyCrawler()

#stocks = ["000001", "000002"]
#for stock in stocks:
#    print(f"\nğŸ“ˆ è·å–è‚¡ç¥¨ {stock}")
#    df = crawler.fetch_data(stock)
#    print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")